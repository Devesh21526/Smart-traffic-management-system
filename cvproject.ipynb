{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6419036,"sourceType":"datasetVersion","datasetId":3702648},{"sourceId":11186203,"sourceType":"datasetVersion","datasetId":6982915}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git\n%cd yolov5\n!pip install -r requirements.txt\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"zDoi1Y5rUtd3","outputId":"7d4be647-7992-4bdd-d4dd-449c087e138c","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:43:25.128420Z","iopub.execute_input":"2025-03-27T13:43:25.128624Z","iopub.status.idle":"2025-03-27T13:43:32.708983Z","shell.execute_reply.started":"2025-03-27T13:43:25.128594Z","shell.execute_reply":"2025-03-27T13:43:32.707866Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'yolov5'...\nremote: Enumerating objects: 17316, done.\u001b[K\nremote: Counting objects: 100% (30/30), done.\u001b[K\nremote: Compressing objects: 100% (25/25), done.\u001b[K\nremote: Total 17316 (delta 17), reused 5 (delta 5), pack-reused 17286 (from 5)\u001b[K\nReceiving objects: 100% (17316/17316), 16.20 MiB | 20.30 MiB/s, done.\nResolving deltas: 100% (11880/11880), done.\n/kaggle/working/yolov5\nRequirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.43)\nRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.5)\nRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\nRequirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\nRequirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (11.0.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\nCollecting thop>=0.1.1 (from -r requirements.txt (line 14))\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\nCollecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n  Downloading ultralytics-8.3.97-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.12.2)\nRequirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->-r requirements.txt (line 7)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->-r requirements.txt (line 7)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->-r requirements.txt (line 7)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->-r requirements.txt (line 7)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->-r requirements.txt (line 7)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->-r requirements.txt (line 7)) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23.5->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23.5->-r requirements.txt (line 7)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.5->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.23.5->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.23.5->-r requirements.txt (line 7)) (2024.2.0)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nDownloading ultralytics-8.3.97-py3-none-any.whl (949 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: thop, ultralytics-thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.3.97 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nMODEL_PATH = \"/kaggle/input/modelbest/best.pt\"  # Update this if different\nif not os.path.exists(MODEL_PATH):\n    print(f\"âŒ Model not found at {MODEL_PATH}. Please check training results.\")\nelse:\n    print(f\"âœ… Found trained model: {MODEL_PATH}\")\n\n# ğŸš€ Step 4: Run validation on the test dataset to generate confusion matrix\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:46:29.482831Z","iopub.execute_input":"2025-03-27T13:46:29.483139Z","iopub.status.idle":"2025-03-27T13:46:29.489958Z","shell.execute_reply.started":"2025-03-27T13:46:29.483118Z","shell.execute_reply":"2025-03-27T13:46:29.489231Z"}},"outputs":[{"name":"stdout","text":"âœ… Found trained model: /kaggle/input/modelbest/best.pt\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python /kaggle/working/yolov5/val.py \\\n    --weights /kaggle/input/modelbest/best.pt \\\n    --data /kaggle/working/yolov5/data.yaml \\\n    --img 640 \\\n    --task val \\\n    --save-json \\\n    --save-txt \\\n    --conf-thres 0.001\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:58:02.173851Z","iopub.execute_input":"2025-03-27T13:58:02.174255Z","iopub.status.idle":"2025-03-27T13:58:21.565481Z","shell.execute_reply.started":"2025-03-27T13:58:02.174224Z","shell.execute_reply":"2025-03-27T13:58:21.564475Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mdata=/kaggle/working/yolov5/data.yaml, weights=['/kaggle/input/modelbest/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=True, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\nYOLOv5 ğŸš€ v7.0-407-g324bcfd6 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\nFusing layers... \nModel summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/traffic-detection-project/test/labels... 279 images,\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/traffic-detection-project/test is not writeable: [Errno 30] Read-only file system: '/kaggle/input/traffic-detection-project/test/labels.cache.npy'\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.942      0.904      0.945      0.711\n               class_0        279        134      0.985      0.951       0.97      0.768\n               class_1        279         41      0.941      0.902      0.955      0.825\n               class_2        279       1911      0.948      0.944      0.967      0.767\n               class_3        279        650      0.939      0.863      0.928      0.604\n               class_4        279        452      0.896      0.859      0.903       0.59\nSpeed: 0.2ms pre-process, 6.4ms inference, 5.3ms NMS per image at shape (32, 3, 640, 640)\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n\nEvaluating pycocotools mAP... saving runs/val/exp7/best_predictions.json...\nloading annotations into memory...\npycocotools unable to run: [Errno 2] No such file or directory: '/kaggle/working/yolov5/annotations/instances_val2017.json'\nResults saved to \u001b[1mruns/val/exp7\u001b[0m\n279 labels saved to runs/val/exp7/labels\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!find /kaggle/working -name \"data.yaml\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:52:43.863920Z","iopub.execute_input":"2025-03-27T13:52:43.864230Z","iopub.status.idle":"2025-03-27T13:52:43.983790Z","shell.execute_reply.started":"2025-03-27T13:52:43.864199Z","shell.execute_reply":"2025-03-27T13:52:43.983062Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5/data.yaml\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ğŸš€ Step 5: Display the Confusion Matrix\nfrom IPython.display import Image\nimport glob\n\n# Find the confusion matrix image in the output directory\nconf_matrix_path = glob.glob(\"/kaggle/working/runs/val/exp/confusion_matrix.png\")\nif conf_matrix_path:\n    display(Image(filename=conf_matrix_path[0]))\nelse:\n    print(\"âŒ Confusion matrix not found. Check if validation ran correctly.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yaml_content = \"\"\"\ntrain: /kaggle/input/traffic-detection-project/train/images\nval: /kaggle/input/traffic-detection-project/test/images\n\nnc: 5  # Number of classes (update based on your dataset)\nnames: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4']  # Add meaningful names for each class\n\"\"\"\n\nwith open('data.yaml', 'w') as f:\n    f.write(yaml_content)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:53:15.614285Z","iopub.execute_input":"2025-03-27T13:53:15.614641Z","iopub.status.idle":"2025-03-27T13:53:15.619390Z","shell.execute_reply.started":"2025-03-27T13:53:15.614611Z","shell.execute_reply":"2025-03-27T13:53:15.618661Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!python train.py --img 640 --batch 16 --epochs 50 --data data.yaml --weights yolov5s.pt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:20:34.286389Z","iopub.execute_input":"2025-03-27T09:20:34.286627Z","iopub.status.idle":"2025-03-27T10:45:32.525580Z","shell.execute_reply.started":"2025-03-27T09:20:34.286606Z","shell.execute_reply":"2025-03-27T10:45:32.524594Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n2025-03-27 09:20:46.819417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-27 09:20:47.025133: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-27 09:20:47.085978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\nYOLOv5 ğŸš€ v7.0-406-g3bc8a531 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ğŸš€ runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 18.0MB/s]\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 148MB/s]\n\nOverriding model.yaml nc=80 with nc=5\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     26970  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nModel summary: 214 layers, 7033114 parameters, 7033114 gradients, 16.0 GFLOPs\n\nTransferred 343/349 items from yolov5s.pt\n/kaggle/working/yolov5/yolov5/models/common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\n/kaggle/working/yolov5/yolov5/models/common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\nWARNING âš ï¸ DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\nSee Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/traffic-detection-project/train/labels... 5805 ima\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/traffic-detection-project/train is not writeable: [Errno 30] Read-only file system: '/kaggle/input/traffic-detection-project/train/labels.cache.npy'\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/traffic-detection-project/test/labels... 279 images,\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ Cache directory /kaggle/input/traffic-detection-project/test is not writeable: [Errno 30] Read-only file system: '/kaggle/input/traffic-detection-project/test/labels.cache.npy'\n\n\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.05 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\nPlotting labels to runs/train/exp/labels.jpg... \n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/kaggle/working/yolov5/yolov5/train.py:355: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=amp)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/train/exp\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n  0%|          | 0/363 [00:00<?, ?it/s]/kaggle/working/yolov5/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/49      1.92G     0.1252    0.06515    0.05427        229        640:  /kaggle/working/yolov5/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/49      1.98G     0.1236     0.0661    0.05441        195        640:  /kaggle/working/yolov5/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/49      1.98G     0.1234    0.06634    0.05443        211        640:  /kaggle/working/yolov5/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/49      1.98G      0.124       0.07    0.05444        323        640:  /kaggle/working/yolov5/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/49      1.98G     0.1242     0.0746    0.05439        344        640:  /kaggle/working/yolov5/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/49      1.98G    0.07859     0.0802     0.0287        142        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.531      0.411      0.292      0.133\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       1/49       2.8G     0.0565    0.07013    0.01655        192        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.435      0.616      0.556      0.282\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       2/49       2.8G    0.05074    0.06582    0.01138        158        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188       0.76      0.684      0.743      0.398\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       3/49       2.8G    0.04458    0.06224   0.008025        278        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.833      0.752       0.83      0.483\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       4/49       2.8G    0.04075    0.05995   0.006152        154        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.832      0.796      0.847      0.524\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       5/49       2.8G    0.03833    0.05806   0.005304        274        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.853      0.811       0.87      0.535\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       6/49       2.8G    0.03654    0.05723   0.004741        242        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.858      0.818      0.875      0.546\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       7/49       2.8G    0.03579     0.0567   0.004328        222        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.876      0.831      0.892      0.583\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       8/49       2.8G    0.03462    0.05542   0.004042        246        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.868      0.842      0.898      0.584\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       9/49       2.8G    0.03357    0.05465   0.003707        280        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.883      0.853        0.9      0.574\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      10/49       2.8G    0.03283    0.05308   0.003444        194        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.894      0.852      0.899      0.592\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      11/49       2.8G    0.03212     0.0536   0.003344        220        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.908      0.845      0.899      0.596\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      12/49       2.8G    0.03155    0.05238    0.00314        162        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.894      0.854      0.911       0.61\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      13/49       2.8G    0.03075    0.05155   0.002988        199        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.903      0.857      0.904      0.606\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      14/49       2.8G    0.03054    0.05086   0.002829        201        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.899      0.878      0.918      0.621\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      15/49       2.8G    0.02989    0.05013   0.002699        196        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.892      0.872      0.914      0.622\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      16/49       2.8G     0.0296       0.05   0.002596        302        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188       0.91      0.856      0.918      0.628\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      17/49       2.8G    0.02935    0.04923   0.002472        273        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.905       0.88      0.927      0.638\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      18/49       2.8G    0.02873    0.04884   0.002482        257        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188       0.92      0.885      0.928      0.641\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      19/49       2.8G    0.02852    0.04811    0.00229        239        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.929      0.879      0.919      0.647\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      20/49       2.8G    0.02822    0.04837    0.00229        197        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.929      0.881      0.932      0.649\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      21/49       2.8G    0.02794    0.04771   0.002222        283        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.927      0.884       0.93      0.647\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      22/49       2.8G    0.02761    0.04713   0.002184        197        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.916      0.886      0.931      0.654\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      23/49       2.8G    0.02736    0.04656   0.002168        212        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.912      0.895      0.933       0.65\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      24/49       2.8G    0.02715    0.04707   0.002012        155        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.923      0.887       0.93      0.657\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      25/49       2.8G    0.02699    0.04664   0.002049        131        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.936      0.901      0.935      0.663\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      26/49       2.8G    0.02633    0.04548    0.00196        205        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.923        0.9      0.936      0.663\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      27/49       2.8G    0.02631    0.04544   0.001878        299        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.914      0.897      0.932      0.666\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      28/49       2.8G    0.02612    0.04538   0.001917        198        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.938      0.887      0.934      0.673\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      29/49       2.8G    0.02601    0.04493   0.001806        194        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.927      0.894      0.937      0.675\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      30/49       2.8G     0.0256     0.0444   0.001737        224        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.934      0.903      0.936      0.675\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      31/49       2.8G    0.02555    0.04411   0.001736        225        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.931      0.887      0.938      0.674\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      32/49       2.8G    0.02513     0.0435   0.001648        259        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.926      0.905       0.94       0.68\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      33/49       2.8G    0.02512     0.0439   0.001644        153        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.926      0.901      0.938      0.686\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      34/49       2.8G    0.02502    0.04332   0.001646        323        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.935      0.895       0.94      0.684\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      35/49       2.8G    0.02475    0.04314   0.001554        169        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.928      0.903       0.94      0.688\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      36/49       2.8G     0.0245     0.0421   0.001577        204        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.937      0.903       0.94      0.691\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      37/49       2.8G    0.02427    0.04188   0.001574        233        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.938      0.886       0.94       0.69\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      38/49       2.8G    0.02426    0.04227   0.001473        179        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188       0.93      0.906      0.942      0.689\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      39/49       2.8G      0.024    0.04199   0.001463        381        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.944       0.89      0.939      0.691\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      40/49       2.8G    0.02385    0.04081   0.001412        259        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.937      0.902      0.941      0.694\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      41/49       2.8G    0.02375    0.04128   0.001388        281        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.941      0.907      0.943      0.698\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      42/49       2.8G    0.02351    0.04111   0.001313        239        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.925      0.909      0.941      0.695\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      43/49       2.8G     0.0236    0.04121   0.001301        227        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.934      0.902       0.94        0.7\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      44/49       2.8G    0.02332    0.04078     0.0013        231        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.942        0.9      0.941      0.701\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      45/49       2.8G    0.02317    0.04005   0.001278        179        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.947      0.905      0.944      0.706\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      46/49       2.8G    0.02312    0.04023   0.001283        182        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.948      0.896      0.944      0.705\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      47/49       2.8G    0.02308    0.04015   0.001263        191        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188       0.94      0.906      0.942      0.706\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      48/49       2.8G    0.02286    0.03996    0.00123        181        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.943      0.904      0.946       0.71\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      49/49       2.8G    0.02286    0.04027    0.00123        199        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.945      0.907      0.945       0.71\n\n50 epochs completed in 1.387 hours.\nOptimizer stripped from runs/train/exp/weights/last.pt, 14.5MB\nOptimizer stripped from runs/train/exp/weights/best.pt, 14.5MB\n\nValidating runs/train/exp/weights/best.pt...\nFusing layers... \nModel summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   \n                   all        279       3188      0.943      0.904      0.946       0.71\n               class_0        279        134      0.985       0.95      0.976      0.766\n               class_1        279         41      0.941      0.902      0.955      0.825\n               class_2        279       1911      0.948      0.944      0.967      0.766\n               class_3        279        650      0.943      0.865      0.929      0.604\n               class_4        279        452      0.898      0.858      0.903       0.59\nResults saved to \u001b[1mruns/train/exp\u001b[0m\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import shutil\n\n# Define source and destination paths\nsource_path = '/kaggle/working/yolov5/runs/train/traffic_detection/weights/best.pt'\ndestination_path = '/kaggle/working/best_model.pt'\n\n# Copy the file to a desired location\nshutil.copy(source_path, destination_path)\n\nprint(f\"Model saved to {destination_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:45:32.526999Z","iopub.execute_input":"2025-03-27T10:45:32.527302Z","iopub.status.idle":"2025-03-27T10:45:32.601768Z","shell.execute_reply.started":"2025-03-27T10:45:32.527280Z","shell.execute_reply":"2025-03-27T10:45:32.600291Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-644266457364>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Copy the file to a desired location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model saved to {destination_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/yolov5/runs/train/traffic_detection/weights/best.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/yolov5/runs/train/traffic_detection/weights/best.pt'","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"!python val.py --data data.yaml --weights /kaggle/working/yolov5/yolov5s.pt --img 640\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:45:32.602194Z","iopub.status.idle":"2025-03-27T10:45:32.602443Z","shell.execute_reply":"2025-03-27T10:45:32.602333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Data from table\nvehicles = [ 'People', 'Cycle', 'Car', 'SUV', 'Truck']\ninstances = [ 134, 41, 1911, 650, 452]\n\n# Plotting the bar chart\nplt.figure(figsize=(8, 5))\nplt.bar(vehicles, instances, color=[ 'green', 'red', 'purple', 'orange', 'cyan'])\n\n# Adding labels and title\nplt.xlabel('Vehicles')\nplt.ylabel('Instances')\nplt.title('Instances per Vehicle Category')\n\n# Display the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:09:41.065323Z","iopub.execute_input":"2025-03-27T15:09:41.065605Z","iopub.status.idle":"2025-03-27T15:09:41.226137Z","shell.execute_reply.started":"2025-03-27T15:09:41.065586Z","shell.execute_reply":"2025-03-27T15:09:41.225225Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport requests\nfrom PIL import Image\n\nfrom super_gradients.training import Trainer, dataloaders, models\nfrom super_gradients.training.losses import PPYoloELoss\nfrom super_gradients.training.metrics import DetectionMetrics_050\n\nfrom super_gradients.training.dataloaders.dataloaders import (\n    coco_detection_yolo_format_train, \n    coco_detection_yolo_format_val\n)\n\nfrom super_gradients.training.models.detection_models.pp_yolo_e import (\n    PPYoloEPostPredictionCallback\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class config:\n    #trainer params\n    CHECKPOINT_DIR = 'checkpoints' #specify the path you want to save checkpoints to\n    EXPERIMENT_NAME = 'finding-battleships' #specify the experiment name\n    #dataset params\n    DATA_DIR = '/kaggle/input/traffic-detection-project' #parent directory to where data lives\n    TRAIN_IMAGES_DIR = 'train/images' #child dir of DATA_DIR where train images are\n    TRAIN_LABELS_DIR = 'train/labels' #child dir of DATA_DIR where train labels are\n    VAL_IMAGES_DIR = 'valid/images' #child dir of DATA_DIR where validation images are\n    VAL_LABELS_DIR = 'valid/labels' #child dir of DATA_DIR where validation labels are\n    TEST_IMAGES_DIR = 'test/images' #child dir of DATA_DIR where test images are\n    TEST_LABELS_DIR = 'test/labels' #child dir of DATA_DIR where test labels are\n\n    CLASSES = ['bicycle', 'bus', 'car', 'motorbike', 'person']\n\n    NUM_CLASSES = len(CLASSES)\n\n    #dataloader params - you can add whatever PyTorch dataloader params you have\n    #could be different across train, val, and test\n    DATALOADER_PARAMS={\n    'batch_size':16,\n    'num_workers':2\n    }\n\n    # model params\n    MODEL_NAME = 'yolo_nas_l' # choose from yolo_nas_s, yolo_nas_m, yolo_nas_l\n    PRETRAINED_WEIGHTS = 'coco' #only one option here: coco","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\n\n# Load YOLOv5 model\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', path='/kaggle/input/modelbest/best.pt')  # Replace 'best.pt' with the path to your trained model\n\n# Define test input images\ntest_images = [\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg',  # Replace with the path to your test images\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg',\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1100_png_jpg.rf.8b7574e1c4f3fd7b654c2a693404fd2d.jpg',\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1240_png_jpg.rf.7e110b54d205ef0537ddc5dec81a79c2.jpg'\n]\n\n# Perform inference on test images\nresults = model(test_images)\n\n# Print results\nresults.print()  # Prints detected objects in console\n\n# Save results with bounding boxes marked on images\nresults.save(save_dir='output/')  # Saves output images in 'output/' directory\n\n# Calculate and print the number of vehicles in each image\nfor i, df in enumerate(results.pandas().xyxy):\n    # Filter for vehicle classes (you may need to adjust these based on your specific model's classes)\n    vehicle_classes = ['car', 'truck', 'bus', 'motorcycle', 'bicycle']\n    \n    # Count vehicles in the current image\n    vehicle_count = df[df['name'].isin(vehicle_classes)].shape[0]\n    \n    print(f\"\\nImage: {test_images[i]}\")\n    print(f\"Total number of vehicles detected: {vehicle_count}\")\n    \n    # Optional: Detailed breakdown by vehicle type\n    vehicle_breakdown = df[df['name'].isin(vehicle_classes)]['name'].value_counts()\n    print(\"Vehicle breakdown:\")\n    print(vehicle_breakdown)\n\n# Optionally, access detailed results as pandas DataFrame\ndf_results = results.pandas().xyxy  # Bounding box coordinates, confidence, class, and name for each detection\nfor i, df in enumerate(df_results):\n    print(f\"\\nDetailed Results for {test_images[i]}:\")\n    print(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:16:03.871556Z","iopub.execute_input":"2025-03-28T09:16:03.871837Z","iopub.status.idle":"2025-03-28T09:16:20.617581Z","shell.execute_reply.started":"2025-03-28T09:16:03.871802Z","shell.execute_reply":"2025-03-28T09:16:20.616757Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n  warnings.warn(\nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","output_type":"stream"},{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"YOLOv5 ğŸš€ 2025-3-28 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\nFusing layers... \nModel summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\nAdding AutoShape... \n/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\nimage 1/4: 640x640 2 class_0s, 17 class_2s, 1 class_3, 1 class_4\nimage 2/4: 640x640 1 class_0, 18 class_2s, 5 class_3s\nimage 3/4: 640x640 1 class_0, 14 class_2s, 3 class_3s\nimage 4/4: 640x640 12 class_2s, 5 class_3s\nSpeed: 28.2ms pre-process, 15.4ms inference, 70.6ms NMS per image at shape (4, 3, 640, 640)\nSaved 4 images to \u001b[1moutput\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Results for /kaggle/input/traffic-detection-project/test/images/aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   476.774078  145.806244  516.952759  200.567810    0.921648      2  class_2\n1   506.786041  202.375092  640.000000  363.409637    0.913485      4  class_4\n2   393.195312  109.812485  418.429565  152.934158    0.884666      2  class_2\n3   356.059570  108.458374  381.584412  150.146088    0.873069      2  class_2\n4   330.891968  501.846680  354.175720  615.166382    0.858714      0  class_0\n5   304.865295  521.192139  332.155151  634.214111    0.856769      0  class_0\n6   402.059357   90.409271  423.582550  124.244797    0.856690      2  class_2\n7   401.603394  151.588837  414.140442  199.006927    0.854808      3  class_3\n8   382.558075   66.199959  403.418610   99.963890    0.827636      2  class_2\n9   392.431519   53.678886  409.859497   80.918770    0.826205      2  class_2\n10  371.845184   87.113472  393.041779  121.054985    0.808403      2  class_2\n11  404.094177   23.141335  422.964111   48.333183    0.780545      2  class_2\n12  400.045013   38.456230  417.829315   63.694618    0.779226      2  class_2\n13  510.741089   50.545124  525.559326   74.849716    0.774595      2  class_2\n14  411.543060   64.967857  428.362396   94.027565    0.759081      2  class_2\n15  419.262207   47.625214  437.317383   76.028214    0.619973      2  class_2\n16  428.532318   26.087902  444.165863   50.987896    0.553896      2  class_2\n17  419.335815    0.158844  431.050232   12.812731    0.517477      2  class_2\n18  422.596588   40.507858  440.687836   68.536743    0.408886      2  class_2\n19  410.001312   17.352306  426.345673   39.593128    0.395601      2  class_2\n20  431.334137   18.365393  446.963409   44.403145    0.269854      2  class_2\nResults for /kaggle/input/traffic-detection-project/test/images/aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   486.156982  490.032898  607.837036  639.820129    0.964436      2  class_2\n1   437.785614  296.395935  507.300262  408.437866    0.939641      2  class_2\n2   351.232544  270.473389  409.241516  357.047913    0.937282      2  class_2\n3   319.930603  182.038193  351.891541  242.587662    0.919033      2  class_2\n4   119.843033  541.294983  144.084518  638.492859    0.904120      3  class_3\n5   366.409241  201.699112  399.255859  272.259216    0.886518      2  class_2\n6   376.037476  140.245163  409.247925  199.041946    0.882871      2  class_2\n7   413.365814   58.734245  432.472931   94.774010    0.880198      2  class_2\n8   399.912598   95.993958  423.159485  135.610703    0.879670      2  class_2\n9   379.090454   69.723083  402.432556  109.105270    0.866819      2  class_2\n10  393.875000   46.510105  412.639404   74.903900    0.857289      2  class_2\n11  234.712982  200.767700  258.156952  254.409210    0.831523      0  class_0\n12  425.955414   34.719906  441.537506   62.466808    0.824550      2  class_2\n13  409.446655   22.858284  423.389526   46.238144    0.814046      2  class_2\n14  510.454041   50.324867  525.641174   75.188934    0.805986      2  class_2\n15  427.822388  146.916733  437.580444  195.736435    0.780912      3  class_3\n16  416.991364    2.449348  430.231598   25.948902    0.666005      2  class_2\n17  135.105606  185.230804  148.323807  228.587860    0.604112      3  class_3\n18  156.834976  179.639999  169.002182  222.337265    0.572608      3  class_3\n19  433.022095   13.050429  446.131531   36.159931    0.492122      2  class_2\n20  131.351318  189.259186  143.839630  232.782379    0.387546      3  class_3\n21  396.593048   47.456100  424.118500   81.346909    0.349850      2  class_2\n22  418.398224    0.020696  430.743988   14.932211    0.288054      2  class_2\n23  328.357239  192.045502  386.459656  264.527435    0.250555      2  class_2\nResults for /kaggle/input/traffic-detection-project/test/images/aguanambi-1100_png_jpg.rf.8b7574e1c4f3fd7b654c2a693404fd2d.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   430.681580  288.987122  502.417542  380.860596    0.945411      2  class_2\n1   325.619812  240.539856  368.493958  332.080933    0.933996      2  class_2\n2   338.298798  144.084732  367.839386  193.012894    0.900534      2  class_2\n3   387.131805  125.953514  414.508820  175.780930    0.879905      2  class_2\n4   370.778412  169.138901  403.986664  238.488754    0.879899      2  class_2\n5   408.581787   61.400749  428.795837  100.095924    0.845919      2  class_2\n6   389.404999   51.520370  409.160492   84.218567    0.845842      2  class_2\n7   392.329651   89.376404  412.640137  125.294037    0.842052      2  class_2\n8   234.202255  200.821030  258.644562  257.018066    0.829768      0  class_0\n9   403.251556   29.011898  420.321503   54.732517    0.817788      2  class_2\n10  510.679596   50.691277  525.584778   75.704124    0.814422      2  class_2\n11  421.707123   37.054531  439.637238   65.854874    0.808737      2  class_2\n12  194.884109  182.256821  208.470078  230.395493    0.762972      3  class_3\n13  418.099121    5.853342  430.525940   30.599821    0.716407      2  class_2\n14  424.346405  155.986008  431.230682  200.468460    0.709799      3  class_3\n15  177.984161  173.446793  190.903839  220.466965    0.524973      3  class_3\n16  432.583252   11.859400  445.651062   35.139107    0.434020      2  class_2\n17  404.927032   32.811714  436.167450   60.414177    0.317171      2  class_2\nResults for /kaggle/input/traffic-detection-project/test/images/aguanambi-1240_png_jpg.rf.7e110b54d205ef0537ddc5dec81a79c2.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   110.011032  391.284882  208.522629  525.499512    0.961403      2  class_2\n1   184.886032  277.670868  245.405045  387.376434    0.944018      2  class_2\n2   226.909271  207.967178  272.917450  284.774841    0.934212      2  class_2\n3   237.655655  154.467865  269.274109  212.255249    0.879225      2  class_2\n4   259.707916  113.431992  287.858551  160.462387    0.857856      2  class_2\n5   482.485260  350.825348  500.111725  428.417328    0.852306      3  class_3\n6   211.637955   90.530014  238.825424  138.765457    0.834423      2  class_2\n7   232.475204   56.241035  253.238907   89.409752    0.832841      2  class_2\n8   504.034790  216.349640  516.272156  270.881104    0.822585      3  class_3\n9   203.356400   46.741367  221.434402   76.210205    0.821706      2  class_2\n10  220.447006   30.526955  237.608597   56.698776    0.808785      2  class_2\n11  113.696693   52.518681  129.326279   78.540253    0.778989      2  class_2\n12  324.435913   87.360794  334.028625  127.021835    0.742150      3  class_3\n13  209.723892    0.558694  222.722305   21.008144    0.637560      2  class_2\n14  199.321442   23.809467  212.405792   51.625095    0.600902      2  class_2\n15  511.077515  216.647812  523.668701  271.890198    0.321212      3  class_3\n16  190.682190   66.720238  196.775116   96.917900    0.317086      3  class_3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom PIL import Image\n\n# Load YOLOv5 model\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', path='/kaggle/input/modelbest/best.pt')  # Replace 'best.pt' with the path to your trained model\n\n# Define test input images\ntest_images = [\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg',  # Replace with the path to your test images\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg',\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1100_png_jpg.rf.8b7574e1c4f3fd7b654c2a693404fd2d.jpg',\n    '/kaggle/input/traffic-detection-project/test/images/aguanambi-1240_png_jpg.rf.7e110b54d205ef0537ddc5dec81a79c2.jpg'\n]\n\n# Perform inference on test images\nresults = model(test_images)\n\n# Print results\nresults.print()  # Prints detected objects in console\n\n# Save results with bounding boxes marked on images\nresults.save(save_dir='output/')  # Saves output images in 'output/' directory\n\n# Calculate and print the number of vehicles in each image\nfor i, df in enumerate(results.pandas().xyxy):\n    # Filter for vehicle classes (you may need to adjust these based on your specific model's classes)\n    vehicle_classes = ['car', 'truck', 'bus', 'motorcycle', 'bicycle']\n    \n    # Count vehicles in the current image\n    vehicle_count = df[df['name'].isin(vehicle_classes)].shape[0]\n    \n    print(f\"\\nImage: {test_images[i]}\")\n    print(f\"Total number of vehicles detected: {vehicle_count}\")\n    \n    # Optional: Detailed breakdown by vehicle type\n    vehicle_breakdown = df[df['name'].isin(vehicle_classes)]['name'].value_counts()\n    print(\"Vehicle breakdown:\")\n    print(vehicle_breakdown)\n\n# Optionally, access detailed results as pandas DataFrame\ndf_results = results.pandas().xyxy  # Bounding box coordinates, confidence, class, and name for each detection\nfor i, df in enumerate(df_results):\n    print(f\"\\nDetailed Results for {test_images[i]}:\")\n    print(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:06:54.699371Z","iopub.execute_input":"2025-03-28T10:06:54.699677Z","iopub.status.idle":"2025-03-28T10:07:18.995041Z","shell.execute_reply.started":"2025-03-28T10:06:54.699655Z","shell.execute_reply":"2025-03-28T10:07:18.994028Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n  warnings.warn(\nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","output_type":"stream"},{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"YOLOv5 ğŸš€ 2025-3-28 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\nFusing layers... \nModel summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\nAdding AutoShape... \n/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\nimage 1/4: 640x640 2 class_0s, 17 class_2s, 1 class_3, 1 class_4\nimage 2/4: 640x640 1 class_0, 18 class_2s, 5 class_3s\nimage 3/4: 640x640 1 class_0, 14 class_2s, 3 class_3s\nimage 4/4: 640x640 12 class_2s, 5 class_3s\nSpeed: 47.4ms pre-process, 29.6ms inference, 144.0ms NMS per image at shape (4, 3, 640, 640)\nSaved 4 images to \u001b[1moutput\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\nImage: /kaggle/input/traffic-detection-project/test/images/aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg\nTotal number of vehicles detected: 0\nVehicle breakdown:\nSeries([], Name: count, dtype: int64)\n\nImage: /kaggle/input/traffic-detection-project/test/images/aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg\nTotal number of vehicles detected: 0\nVehicle breakdown:\nSeries([], Name: count, dtype: int64)\n\nImage: /kaggle/input/traffic-detection-project/test/images/aguanambi-1100_png_jpg.rf.8b7574e1c4f3fd7b654c2a693404fd2d.jpg\nTotal number of vehicles detected: 0\nVehicle breakdown:\nSeries([], Name: count, dtype: int64)\n\nImage: /kaggle/input/traffic-detection-project/test/images/aguanambi-1240_png_jpg.rf.7e110b54d205ef0537ddc5dec81a79c2.jpg\nTotal number of vehicles detected: 0\nVehicle breakdown:\nSeries([], Name: count, dtype: int64)\n\nDetailed Results for /kaggle/input/traffic-detection-project/test/images/aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   476.774078  145.806244  516.952759  200.567810    0.921648      2  class_2\n1   506.786041  202.375092  640.000000  363.409637    0.913485      4  class_4\n2   393.195312  109.812485  418.429565  152.934158    0.884666      2  class_2\n3   356.059570  108.458374  381.584412  150.146088    0.873069      2  class_2\n4   330.891968  501.846680  354.175720  615.166382    0.858714      0  class_0\n5   304.865295  521.192139  332.155151  634.214111    0.856769      0  class_0\n6   402.059357   90.409271  423.582550  124.244797    0.856690      2  class_2\n7   401.603394  151.588837  414.140442  199.006927    0.854808      3  class_3\n8   382.558075   66.199959  403.418610   99.963890    0.827636      2  class_2\n9   392.431519   53.678886  409.859497   80.918770    0.826205      2  class_2\n10  371.845184   87.113472  393.041779  121.054985    0.808403      2  class_2\n11  404.094177   23.141335  422.964111   48.333183    0.780545      2  class_2\n12  400.045013   38.456230  417.829315   63.694618    0.779226      2  class_2\n13  510.741089   50.545124  525.559326   74.849716    0.774595      2  class_2\n14  411.543060   64.967857  428.362396   94.027565    0.759081      2  class_2\n15  419.262207   47.625214  437.317383   76.028214    0.619973      2  class_2\n16  428.532318   26.087902  444.165863   50.987896    0.553896      2  class_2\n17  419.335815    0.158844  431.050232   12.812731    0.517477      2  class_2\n18  422.596588   40.507858  440.687836   68.536743    0.408886      2  class_2\n19  410.001312   17.352306  426.345673   39.593128    0.395601      2  class_2\n20  431.334137   18.365393  446.963409   44.403145    0.269854      2  class_2\n\nDetailed Results for /kaggle/input/traffic-detection-project/test/images/aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   486.156982  490.032898  607.837036  639.820129    0.964436      2  class_2\n1   437.785614  296.395935  507.300262  408.437866    0.939641      2  class_2\n2   351.232544  270.473389  409.241516  357.047913    0.937282      2  class_2\n3   319.930603  182.038193  351.891541  242.587662    0.919033      2  class_2\n4   119.843033  541.294983  144.084518  638.492859    0.904120      3  class_3\n5   366.409241  201.699112  399.255859  272.259216    0.886518      2  class_2\n6   376.037476  140.245163  409.247925  199.041946    0.882871      2  class_2\n7   413.365814   58.734245  432.472931   94.774010    0.880198      2  class_2\n8   399.912598   95.993958  423.159485  135.610703    0.879670      2  class_2\n9   379.090454   69.723083  402.432556  109.105270    0.866819      2  class_2\n10  393.875000   46.510105  412.639404   74.903900    0.857289      2  class_2\n11  234.712982  200.767700  258.156952  254.409210    0.831523      0  class_0\n12  425.955414   34.719906  441.537506   62.466808    0.824550      2  class_2\n13  409.446655   22.858284  423.389526   46.238144    0.814046      2  class_2\n14  510.454041   50.324867  525.641174   75.188934    0.805986      2  class_2\n15  427.822388  146.916733  437.580444  195.736435    0.780912      3  class_3\n16  416.991364    2.449348  430.231598   25.948902    0.666005      2  class_2\n17  135.105606  185.230804  148.323807  228.587860    0.604112      3  class_3\n18  156.834976  179.639999  169.002182  222.337265    0.572608      3  class_3\n19  433.022095   13.050429  446.131531   36.159931    0.492122      2  class_2\n20  131.351318  189.259186  143.839630  232.782379    0.387546      3  class_3\n21  396.593048   47.456100  424.118500   81.346909    0.349850      2  class_2\n22  418.398224    0.020696  430.743988   14.932211    0.288054      2  class_2\n23  328.357239  192.045502  386.459656  264.527435    0.250555      2  class_2\n\nDetailed Results for /kaggle/input/traffic-detection-project/test/images/aguanambi-1100_png_jpg.rf.8b7574e1c4f3fd7b654c2a693404fd2d.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   430.681580  288.987122  502.417542  380.860596    0.945411      2  class_2\n1   325.619812  240.539856  368.493958  332.080933    0.933996      2  class_2\n2   338.298798  144.084732  367.839386  193.012894    0.900534      2  class_2\n3   387.131805  125.953514  414.508820  175.780930    0.879905      2  class_2\n4   370.778412  169.138901  403.986664  238.488754    0.879899      2  class_2\n5   408.581787   61.400749  428.795837  100.095924    0.845919      2  class_2\n6   389.404999   51.520370  409.160492   84.218567    0.845842      2  class_2\n7   392.329651   89.376404  412.640137  125.294037    0.842052      2  class_2\n8   234.202255  200.821030  258.644562  257.018066    0.829768      0  class_0\n9   403.251556   29.011898  420.321503   54.732517    0.817788      2  class_2\n10  510.679596   50.691277  525.584778   75.704124    0.814422      2  class_2\n11  421.707123   37.054531  439.637238   65.854874    0.808737      2  class_2\n12  194.884109  182.256821  208.470078  230.395493    0.762972      3  class_3\n13  418.099121    5.853342  430.525940   30.599821    0.716407      2  class_2\n14  424.346405  155.986008  431.230682  200.468460    0.709799      3  class_3\n15  177.984161  173.446793  190.903839  220.466965    0.524973      3  class_3\n16  432.583252   11.859400  445.651062   35.139107    0.434020      2  class_2\n17  404.927032   32.811714  436.167450   60.414177    0.317171      2  class_2\n\nDetailed Results for /kaggle/input/traffic-detection-project/test/images/aguanambi-1240_png_jpg.rf.7e110b54d205ef0537ddc5dec81a79c2.jpg:\n          xmin        ymin        xmax        ymax  confidence  class     name\n0   110.011032  391.284882  208.522629  525.499512    0.961403      2  class_2\n1   184.886032  277.670868  245.405045  387.376434    0.944018      2  class_2\n2   226.909271  207.967178  272.917450  284.774841    0.934212      2  class_2\n3   237.655655  154.467865  269.274109  212.255249    0.879225      2  class_2\n4   259.707916  113.431992  287.858551  160.462387    0.857856      2  class_2\n5   482.485260  350.825348  500.111725  428.417328    0.852306      3  class_3\n6   211.637955   90.530014  238.825424  138.765457    0.834423      2  class_2\n7   232.475204   56.241035  253.238907   89.409752    0.832841      2  class_2\n8   504.034790  216.349640  516.272156  270.881104    0.822585      3  class_3\n9   203.356400   46.741367  221.434402   76.210205    0.821706      2  class_2\n10  220.447006   30.526955  237.608597   56.698776    0.808785      2  class_2\n11  113.696693   52.518681  129.326279   78.540253    0.778989      2  class_2\n12  324.435913   87.360794  334.028625  127.021835    0.742150      3  class_3\n13  209.723892    0.558694  222.722305   21.008144    0.637560      2  class_2\n14  199.321442   23.809467  212.405792   51.625095    0.600902      2  class_2\n15  511.077515  216.647812  523.668701  271.890198    0.321212      3  class_3\n16  190.682190   66.720238  196.775116   96.917900    0.317086      3  class_3\n","output_type":"stream"}],"execution_count":1}]}